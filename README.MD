# ğŸ¤– Agent core - Acelerador de Projetos de IA Generativa

**Template profissional para desenvolvimento de projetos de IA generativa usando Clean Architecture.**

Este projeto implementa um agente inteligente completo com arquitetura limpa, integraÃ§Ã£o MCP (Model Context Protocol), base de conhecimento vetorial e sistema de ferramentas extensÃ­vel.

## âœ¨ CaracterÃ­sticas

- ğŸ—ï¸ **Clean Architecture**: Estrutura profissional seguindo padrÃµes da indÃºstria
- ğŸ”„ **Multi-Provider**: Suporte para Ollama, OpenAI e Google Gemini
- ğŸ§  **Base de Conhecimento**: Sistema vetorial com ChromaDB para RAG
- ğŸ› ï¸ **Sistema de Ferramentas**: IntegraÃ§Ã£o com LangChain tools (web search, Wikipedia, calculadora)
- ğŸ“¡ **MCP Integration**: Servidor MCP para integraÃ§Ã£o com Claude Desktop
- ğŸ“ **Logging Profissional**: Sistema de logs estruturado com mÃ©todos especializados para ferramentas e agentes
- âš™ï¸ **ConfiguraÃ§Ã£o Centralizada**: Gerenciamento via variÃ¡veis de ambiente
- ğŸ§ª **Testes Integrados**: Scripts de teste para todos os componentes

## ğŸ—ï¸ Arquitetura Clean

```
ğŸ“ Agent core/
â”œâ”€â”€ ğŸ¯ core/                    # Camada de NegÃ³cio (Domain)
â”‚   â”œâ”€â”€ entities/               # Entidades de domÃ­nio
â”‚   â”‚   â”œâ”€â”€ agent.py           # Modelos do agente
â”‚   â”‚   â””â”€â”€ message.py         # Modelos de mensagem
â”‚   â”œâ”€â”€ use_cases/             # Casos de uso (lÃ³gica de negÃ³cio)
â”‚   â”‚   â””â”€â”€ agent_orchestration.py
â”‚   â””â”€â”€ interfaces/            # Contratos (abstraÃ§Ãµes)
â”‚       â”œâ”€â”€ llm_provider.py
â”‚       â”œâ”€â”€ tool_repository.py
â”‚       â”œâ”€â”€ storage_repository.py
â”‚       â””â”€â”€ agent_interface.py
â”œâ”€â”€ ğŸ”Œ adapters/               # Camada de Adaptadores
â”‚   â”œâ”€â”€ llm/                   # Provedores LLM
â”‚   â”‚   â”œâ”€â”€ providers.py       # ImplementaÃ§Ãµes (Ollama, OpenAI, Gemini)
â”‚   â”‚   â””â”€â”€ factory.py         # Factory pattern
â”‚   â”œâ”€â”€ tools/                 # Ferramentas
â”‚   â”‚   â””â”€â”€ langchain_tool_repository.py
â”‚   â”œâ”€â”€ storage/               # Armazenamento
â”‚   â”‚   â””â”€â”€ vector_store_adapter.py
â”‚   â””â”€â”€ mcp/                   # MCP Integration
â”‚       â””â”€â”€ mcp_server_adapter.py
â”œâ”€â”€ ğŸ¢ infrastructure/         # Camada de Infraestrutura
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ settings.py        # ConfiguraÃ§Ãµes centralizadas
â”œâ”€â”€ ğŸ”§ shared/                 # UtilitÃ¡rios compartilhados
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger.py          # Sistema de logging profissional com mÃ©todos especializados
â”œâ”€â”€ ğŸš€ Entry Points           # Pontos de entrada
â”‚   â”œâ”€â”€ main_clean.py         # CLI com Clean Architecture
â”‚   â”œâ”€â”€ mcp_server_clean.py   # MCP Server com Clean Architecture
â”‚   â”œâ”€â”€ main.py               # CLI legado (compatibilidade)
â”‚   â””â”€â”€ mcp_server.py         # MCP Server legado
â””â”€â”€ ğŸ“Š logs/                  # Logs da aplicaÃ§Ã£o
```

## ğŸš€ ConfiguraÃ§Ã£o

### 1. Instalar dependÃªncias
```bash
pip install -r requirements.txt
```

### 2. Configurar variÃ¡veis de ambiente
```bash
cp .env.example .env
# Edite o arquivo .env com suas configuraÃ§Ãµes
```

### 3. ConfiguraÃ§Ãµes DisponÃ­veis

**ğŸ¤– AplicaÃ§Ã£o:**
- `APP_NAME`: Nome da aplicaÃ§Ã£o (padrÃ£o: Agent core)
- `APP_VERSION`: VersÃ£o (padrÃ£o: 1.0.0)
- `ENVIRONMENT`: Ambiente (development/production)
- `DEBUG`: Modo debug (true/false)

**ğŸ§  Provedor LLM:**
- `MAIN_PROVIDER`: Provedor principal ('ollama', 'openai', 'gemini')
- `OLLAMA_BASE_URL`: URL do Ollama (padrÃ£o: http://localhost:11434)
- `OLLAMA_MODEL`: Modelo Ollama (padrÃ£o: llama3:latest)
- `OLLAMA_EMBEDDINGS_MODEL`: Modelo embeddings (padrÃ£o: nomic-embed-text)
- `OPENAI_API_KEY`: Chave API OpenAI
- `OPENAI_MODEL`: Modelo OpenAI (padrÃ£o: gpt-3.5-turbo)
- `GEMINI_API_KEY`: Chave API Gemini
- `GEMINI_MODEL`: Modelo Gemini (padrÃ£o: gemini-1.5-flash)

**ğŸ’¾ Base de Conhecimento:**
- `CHROMA_PERSIST_DIRECTORY`: DiretÃ³rio ChromaDB (padrÃ£o: ./data/vector_stores)
- `VECTOR_SEARCH_K_RESULTS`: NÃºmero de resultados (padrÃ£o: 3)
- `VECTOR_COLLECTION_NAME`: Nome da coleÃ§Ã£o (padrÃ£o: ai_accelerator_knowledge)

**ğŸ“¡ MCP Server:**
- `MCP_SERVER_NAME`: Nome do servidor (padrÃ£o: AgentCoreMCP)
- `MCP_HOST`: Host (padrÃ£o: 127.0.0.1)
- `MCP_PORT`: Porta (padrÃ£o: 8088)
- `MCP_TRANSPORT`: Transporte (padrÃ£o: sse)

**ğŸ“ Logging:**
- `LOG_LEVEL`: NÃ­vel de log (INFO, DEBUG, ERROR)
- `LOG_DIR`: DiretÃ³rio dos logs (padrÃ£o: ./logs)
- `LOG_MAX_FILE_SIZE`: Tamanho mÃ¡ximo do arquivo (padrÃ£o: 10MB)
- `LOG_BACKUP_COUNT`: NÃºmero de backups (padrÃ£o: 5)

**Sistema de Logging AvanÃ§ado:**
O logger inclui mÃ©todos especializados:
- `tool_execution()`: Para logs de execuÃ§Ã£o de ferramentas
- `tool_success()` / `tool_error()`: Para resultados de ferramentas
- `knowledge_search()`: Para buscas na base de conhecimento
- `agent_decision()`: Para decisÃµes do agente
- `success()` / `progress()`: Para indicadores visuais

## ğŸ¯ Guia de Uso

### ğŸš€ Quick Start

**1. Executar CLI com Clean Architecture:**
```bash
python main_clean.py
```

**2. Executar MCP Server com Clean Architecture:**
```bash
mcp dev mcp_server_clean.py
```

### âš™ï¸ ConfiguraÃ§Ã£o de Provedores

**Para Ollama (Recomendado para desenvolvimento):**
```bash
# 1. Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 2. Iniciar servidor
ollama serve

# 3. Baixar modelos
ollama pull llama3:latest
ollama pull nomic-embed-text

# 4. Configurar .env
MAIN_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3:latest
```

**Para OpenAI:**
```bash
# 1. Configurar .env
MAIN_PROVIDER=openai
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-3.5-turbo

# 2. Instalar dependÃªncia
pip install langchain-openai
```

**Para Google Gemini:**
```bash
# 1. Configurar .env
MAIN_PROVIDER=gemini
GEMINI_API_KEY=...
GEMINI_MODEL=gemini-1.5-flash

# 2. Instalar dependÃªncia
pip install langchain-google-genai
```

### ğŸ§ª Testes e ValidaÃ§Ã£o

**Testar configuraÃ§Ã£o completa:**
```bash
# Testar provedores LLM
python -c "from infrastructure.config.settings import settings; print('âœ… ConfiguraÃ§Ã£o vÃ¡lida')"

# Testar CLI Clean Architecture
python main_clean.py
# Digite: "olÃ¡, como vocÃª estÃ¡?"

# Testar MCP Server
mcp dev mcp_server_clean.py
```

**Usar MCP Inspector (Interface Web):**
```bash
# 1. Instalar MCP Inspector
npm install -g @mcp/inspector

# 2. Em terminal separado, executar MCP server
mcp dev mcp_server_clean.py

# 3. Em outro terminal, executar inspector
mcp-inspector

# 4. Abrir http://localhost:3000 no navegador
# 5. Conectar em: stdio ou sse://127.0.0.1:8088
```

### ğŸ› ï¸ Ferramentas DisponÃ­veis

O sistema inclui as seguintes ferramentas integradas:

- **ğŸ” search_knowledge_base**: Busca na base de conhecimento vetorial
- **ğŸŒ web_search**: Busca na web usando DuckDuckGo
- **ğŸ“š wikipedia_search**: Busca na Wikipedia
- **ğŸ§® calculator**: Calculadora matemÃ¡tica segura
- **âš™ï¸ get_system_status**: Status do sistema e configuraÃ§Ãµes
- **ğŸ”— test_agent_connection**: Teste de conectividade dos provedores

### ğŸ“– Exemplos de Uso

**InteraÃ§Ã£o bÃ¡sica:**
```
ğŸ‘¤ VocÃª: Qual Ã© a capital do Brasil?
ğŸ¤– Agent: A capital do Brasil Ã© BrasÃ­lia. [Ferramentas usadas: web_search]
```

**Busca com mÃºltiplas ferramentas:**
```
ğŸ‘¤ VocÃª: Pesquise sobre machine learning e calcule 2+2
ğŸ¤– Agent: Machine learning Ã©... [base de conhecimento] 
         Resultado do cÃ¡lculo: 4 
         [Ferramentas usadas: search_knowledge_base, calculator]
```

## ğŸ—ï¸ Arquitetura e Extensibilidade

### ğŸ“‹ PrincÃ­pios da Clean Architecture

1. **ğŸ¯ IndependÃªncia de Frameworks**: Core nÃ£o depende de bibliotecas externas
2. **ğŸ§ª Testabilidade**: LÃ³gica de negÃ³cio isolada e testÃ¡vel
3. **ğŸ”„ IndependÃªncia de UI**: MÃºltiplos pontos de entrada (CLI, MCP, API)
4. **ğŸ’¾ IndependÃªncia de Banco**: Adapters abstraem persistÃªncia
5. **ğŸŒ IndependÃªncia Externa**: Provedores LLM intercambiÃ¡veis

### ğŸ”§ Como Estender

**Adicionar novo provedor LLM:**
```python
# 1. Implementar interface em adapters/llm/providers.py
class NovoProvider(LLMProviderInterface):
    def generate_response(self, prompt: str) -> str:
        # ImplementaÃ§Ã£o especÃ­fica
        pass

# 2. Adicionar ao factory em adapters/llm/factory.py
# 3. Adicionar enum em core/entities/agent.py
# 4. Configurar em infrastructure/config/settings.py
```

**Adicionar nova ferramenta:**
```python
# Em adapters/tools/langchain_tool_repository.py
class NovaFerramenta(BaseTool):
    name = "nova_ferramenta"
    description = "DescriÃ§Ã£o da ferramenta"
    
    def _run(self, query: str) -> str:
        # ImplementaÃ§Ã£o da ferramenta
        return "Resultado"
```

## ğŸ” Monitoramento e Logs

O sistema possui um **sistema de logging profissional** com mÃ©todos especializados:

### ğŸ“ Categorias de Logs
- **ğŸ“± app**: Logs gerais da aplicaÃ§Ã£o
- **ğŸ¤– agent**: Logs do agente e orquestraÃ§Ã£o
- **ğŸ› ï¸ tools**: Logs de execuÃ§Ã£o de ferramentas (com `tool_execution`, `tool_success`, `tool_error`)
- **ğŸ§  llm**: Logs dos provedores LLM
- **ğŸ“¡ mcp**: Logs do servidor MCP
- **ğŸ’¾ storage**: Logs da base de conhecimento

### ğŸ¨ MÃ©todos de Logging Especializados
```python
# Exemplos de uso do logger
logger.tool_execution("search_knowledge_base", query="python")
logger.tool_success("web_search", "Resultados encontrados")
logger.tool_error("calculator", "ExpressÃ£o invÃ¡lida")
logger.knowledge_search("machine learning")
logger.agent_decision("Escolhendo ferramenta web_search")
logger.success("Sistema inicializado com sucesso")
logger.progress("Processando consulta...")
```

### âš™ï¸ ConfiguraÃ§Ã£o de Logs
```bash
# No .env
LOG_LEVEL=DEBUG  # Para desenvolvimento
LOG_LEVEL=INFO   # Para produÃ§Ã£o
```

### ğŸ“Š Formato dos Logs
- **Console**: `HH:MM:SS | LEVEL | component | message | context`
- **Arquivo**: `YYYY-MM-DD HH:MM:SS | LEVEL | component | file:line | message | context`

## ğŸš€ Deploy e ProduÃ§Ã£o

### Docker (Recomendado)
```dockerfile
# Dockerfile exemplo
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "main_clean.py"]
```

### VariÃ¡veis de Ambiente para ProduÃ§Ã£o
```bash
ENVIRONMENT=production
DEBUG=false
LOG_LEVEL=INFO
LOG_ENABLE_CONSOLE=false
LOG_ENABLE_FILE=true
```

## ğŸ”§ ResoluÃ§Ã£o de Problemas

### âŒ ModuleNotFoundError: No module named 'logger_config'
**SoluÃ§Ã£o**: Este erro foi corrigido! O projeto agora usa o sistema de logging unificado em `shared/utils/logger.py`. Se ainda ocorrer, verifique se todos os imports estÃ£o corretos:
```python
# âœ… Correto
from shared.utils.logger import get_logger
logger = get_logger("component")

# âŒ Incorreto (removido)
from logger_config import logger
```

### ğŸ” 'AIAcceleratorLogger' object has no attribute 'tool_error'
**SoluÃ§Ã£o**: Todos os mÃ©todos necessÃ¡rios foram adicionados ao logger:
- `tool_error()`: Para erros de ferramentas
- `tool_success()`: Para sucessos de ferramentas
- MÃ©todos especializados para diferentes contextos

### ğŸ› ï¸ Desenvolvendo Novas Funcionalidades
Para garantir compatibilidade com o sistema de logging:
```python
# Importe o logger corretamente
from shared.utils.logger import get_logger
logger = get_logger("seu_componente")

# Use mÃ©todos apropriados
logger.tool_execution("nome_ferramenta", parametros={"key": "value"})
logger.tool_success("nome_ferramenta", "OperaÃ§Ã£o realizada")
logger.error("Erro especÃ­fico", contexto={"detalhes": "info"})
```

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie sua branch (`git checkout -b feature/nova-funcionalidade`)
3. Commit suas mudanÃ§as (`git commit -m 'Adiciona nova funcionalidade'`)
4. Push para a branch (`git push origin feature/nova-funcionalidade`)
5. Abra um Pull Request

### ğŸ“‹ PadrÃµes de CÃ³digo
- **Logger**: Sempre use `get_logger("componente")` do mÃ³dulo `shared.utils.logger`
- **Arquitetura**: Siga os princÃ­pios da Clean Architecture
- **Testes**: Inclua testes para novas funcionalidades
- **DocumentaÃ§Ã£o**: Atualize o README quando necessÃ¡rio

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob licenÃ§a MIT. Veja o arquivo `LICENSE` para detalhes.

---

**ğŸ¯ Agent core - Acelere seus projetos de IA generativa com arquitetura profissional!**