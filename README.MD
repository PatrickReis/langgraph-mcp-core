# LangGraph Agent com MCP Bridge

Este projeto implementa um agente inteligente usando LangGraph com acesso a uma base de conhecimento vetorial e integra√ß√£o MCP (Model Context Protocol).

## üöÄ Configura√ß√£o

### 1. Instalar depend√™ncias
```bash
pip install -r requirements.txt
```

### 2. Configurar vari√°veis de ambiente
Copie o arquivo `.env.example` para `.env` e ajuste as configura√ß√µes:

```bash
cp .env.example .env
```

#### Configura√ß√µes dispon√≠veis:

**Provedor Principal:**
- `MAIN_PROVIDER`: Provedor LLM principal ('ollama', 'openai', 'gemini') (padr√£o: ollama)

**Ollama:**
- `OLLAMA_BASE_URL`: URL do servidor Ollama (padr√£o: http://localhost:11434)
- `OLLAMA_MODEL`: Modelo LLM principal (padr√£o: llama3:latest)
- `OLLAMA_EMBEDDINGS_MODEL`: Modelo para embeddings (padr√£o: nomic-embed-text)

**OpenAI:**
- `OPENAI_API_KEY`: Sua chave de API da OpenAI
- `OPENAI_MODEL`: Modelo OpenAI (padr√£o: gpt-3.5-turbo)

**Google Gemini:**
- `GEMINI_API_KEY`: Sua chave de API do Google AI Studio
- `GEMINI_MODEL`: Modelo Gemini (padr√£o: gemini-1.5-flash)

**ChromaDB:**
- `CHROMA_PERSIST_DIRECTORY`: Diret√≥rio de persist√™ncia (padr√£o: ./chroma_db)

**Busca Vetorial:**
- `VECTOR_SEARCH_K_RESULTS`: N√∫mero de resultados da busca (padr√£o: 3)

**MCP Server:**
- `MCP_SERVER_NAME`: Nome do servidor MCP (padr√£o: LangGraphToolsMCP)

### 3. Configurar Provedor LLM

**Para Ollama (padr√£o):**
```bash
# O arquivo .env j√° est√° configurado para Ollama
ollama serve
```

**Para OpenAI:**
```bash
# Editar .env
MAIN_PROVIDER=openai
OPENAI_API_KEY=sua_chave_aqui

# Instalar depend√™ncia
pip install langchain-openai
```

**Para Google Gemini:**
```bash
# Editar .env
MAIN_PROVIDER=gemini
GEMINI_API_KEY=sua_chave_aqui

# Instalar depend√™ncia
pip install langchain-google-genai
```

### 4. Testar Provedores
```bash
python test_providers.py
```

## üéØ Uso

### Executar o agente principal
```bash
python main.py
```

### Executar servidor MCP
```bash
fastmcp run mcp_server.py
```

### Executar servidor MCP com HTTP
```bash
fastmcp run mcp_server.py --transport streamable-http --host 127.0.0.1 --port 8088

```

## üîß Funcionalidades

- **Agente Inteligente**: Processa perguntas usando LangGraph
- **Base de Conhecimento**: Busca vetorial com ChromaDB
- **Integra√ß√£o MCP**: Bridge para ferramentas LangChain
- **Configura√ß√£o Flex√≠vel**: Todas as configura√ß√µes via vari√°veis de ambiente
- **M√∫ltiplos Provedores**: Suporte para Ollama, OpenAI e Google Gemini
- **Arquitetura Modular**: Sistema componentizado para f√°cil extens√£o

## üìÅ Estrutura do Projeto

- `main.py`: Agente principal com interface de linha de comando
- `tools.py`: Ferramentas de busca vetorial
- `mcp_server.py`: Servidor MCP
- `mcp_bridge.py`: Bridge entre LangChain e MCP
- `llm_providers.py`: Sistema modular de provedores LLM
- `.env`: Configura√ß√µes do ambiente (n√£o commitado)
- `.env.example`: Exemplo de configura√ß√µes