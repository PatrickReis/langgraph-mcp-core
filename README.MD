# LangGraph Agent com MCP Bridge

Este projeto implementa um agente inteligente usando LangGraph com acesso a uma base de conhecimento vetorial e integra√ß√£o MCP (Model Context Protocol).

## üöÄ Configura√ß√£o

### 1. Instalar depend√™ncias
```bash
pip install -r requirements.txt
```

### 2. Configurar vari√°veis de ambiente
Copie o arquivo `.env.example` para `.env` e ajuste as configura√ß√µes:

```bash
cp .env.example .env
```

#### Configura√ß√µes dispon√≠veis:

**Ollama:**
- `OLLAMA_BASE_URL`: URL do servidor Ollama (padr√£o: http://localhost:11434)
- `OLLAMA_MODEL`: Modelo LLM principal (padr√£o: llama3:latest)
- `OLLAMA_EMBEDDINGS_MODEL`: Modelo para embeddings (padr√£o: nomic-embed-text)

**ChromaDB:**
- `CHROMA_PERSIST_DIRECTORY`: Diret√≥rio de persist√™ncia (padr√£o: ./chroma_db)

**Busca Vetorial:**
- `VECTOR_SEARCH_K_RESULTS`: N√∫mero de resultados da busca (padr√£o: 3)

**MCP Server:**
- `MCP_SERVER_NAME`: Nome do servidor MCP (padr√£o: LangGraphToolsMCP)

### 3. Iniciar Ollama
```bash
ollama serve
```

## üéØ Uso

### Executar o agente principal
```bash
python main.py
```

### Executar servidor MCP
```bash
fastmcp run mcp_server.py
```

### Executar servidor MCP com HTTP
```bash
fastmcp run mcp_server.py --transport streamable-http --host 127.0.0.1 --port 8088
```

## üîß Funcionalidades

- **Agente Inteligente**: Processa perguntas usando LangGraph
- **Base de Conhecimento**: Busca vetorial com ChromaDB
- **Integra√ß√£o MCP**: Bridge para ferramentas LangChain
- **Configura√ß√£o Flex√≠vel**: Todas as configura√ß√µes via vari√°veis de ambiente

## üìÅ Estrutura do Projeto

- `main.py`: Agente principal com interface de linha de comando
- `tools.py`: Ferramentas de busca vetorial
- `mcp_server.py`: Servidor MCP
- `mcp_bridge.py`: Bridge entre LangChain e MCP
- `.env`: Configura√ß√µes do ambiente (n√£o commitado)
- `.env.example`: Exemplo de configura√ß√µes