# LangGraph Agent com MCP Bridge

Este projeto implementa um agente inteligente usando LangGraph com acesso a uma base de conhecimento vetorial e integra√ß√£o MCP (Model Context Protocol).

## üöÄ Configura√ß√£o

### 1. Instalar depend√™ncias
```bash
pip install -r requirements.txt
```

### 2. Configurar vari√°veis de ambiente
Copie o arquivo `.env.example` para `.env` e ajuste as configura√ß√µes:

```bash
cp .env.example .env
```

#### Configura√ß√µes dispon√≠veis:

**Provedor Principal:**
- `MAIN_PROVIDER`: Provedor LLM principal ('ollama', 'openai', 'gemini') (padr√£o: ollama)

**Ollama:**
- `OLLAMA_BASE_URL`: URL do servidor Ollama (padr√£o: http://localhost:11434)
- `OLLAMA_MODEL`: Modelo LLM principal (padr√£o: llama3:latest)
- `OLLAMA_EMBEDDINGS_MODEL`: Modelo para embeddings (padr√£o: nomic-embed-text)

**OpenAI:**
- `OPENAI_API_KEY`: Sua chave de API da OpenAI
- `OPENAI_MODEL`: Modelo OpenAI (padr√£o: gpt-3.5-turbo)

**Google Gemini:**
- `GEMINI_API_KEY`: Sua chave de API do Google AI Studio
- `GEMINI_MODEL`: Modelo Gemini (padr√£o: gemini-1.5-flash)

**ChromaDB:**
- `CHROMA_PERSIST_DIRECTORY`: Diret√≥rio de persist√™ncia (padr√£o: ./chroma_db)

**Busca Vetorial:**
- `VECTOR_SEARCH_K_RESULTS`: N√∫mero de resultados da busca (padr√£o: 3)

**MCP Server:**
- `MCP_SERVER_NAME`: Nome do servidor MCP (padr√£o: LangGraphToolsMCP)

### 3. Configurar Provedor LLM

**Para Ollama (padr√£o):**
```bash
# O arquivo .env j√° est√° configurado para Ollama
ollama serve
```

**Para OpenAI:**
```bash
# Editar .env
MAIN_PROVIDER=openai
OPENAI_API_KEY=sua_chave_aqui

# Instalar depend√™ncia
pip install langchain-openai
```

**Para Google Gemini:**
```bash
# Editar .env
MAIN_PROVIDER=gemini
GEMINI_API_KEY=sua_chave_aqui

# Instalar depend√™ncia
pip install langchain-google-genai
```

### 4. Testar Provedores
```bash
python test_providers.py
```

## üéØ Uso

### Executar o agente principal
```bash
python agent.py
```

### Executar servidor MCP
```bash
fastmcp run mcp_server.py
```

### Executar servidor MCP com HTTP
```bash
fastmcp run mcp_server.py --transport streamable-http --host 127.0.0.1 --port 8088
```

### Testar servidor MCP
```bash
python test_mcp.py
```

### Usar MCP Inspector (Interface Web)
```bash
# Instalar MCP Inspector
pip install mcp-inspector

# Executar (em terminal separado)
mcp-inspector

# Abrir navegador em http://localhost:3000
# Conectar em: http://127.0.0.1:8088/mcp
```

## üîß Funcionalidades

- **Agente Inteligente**: Processa perguntas usando LangGraph
- **Base de Conhecimento**: Busca vetorial com ChromaDB
- **Integra√ß√£o MCP Completa**: Bridge para ferramentas LangChain + Resources + Prompts
- **MCP Resources**: Acesso a configura√ß√µes, status e informa√ß√µes da base de conhecimento
- **MCP Prompts**: Templates predefinidos para diferentes tipos de consultas
- **Orquestra√ß√£o Inteligente**: Decide automaticamente quando usar ferramentas vs resposta direta
- **Configura√ß√£o Flex√≠vel**: Todas as configura√ß√µes via vari√°veis de ambiente
- **M√∫ltiplos Provedores**: Suporte para Ollama, OpenAI e Google Gemini
- **Arquitetura Modular**: Sistema componentizado para f√°cil extens√£o

## üìÅ Estrutura do Projeto

```
langgraph/
‚îú‚îÄ‚îÄ agent.py                     # Agente principal com interface de linha de comando
‚îú‚îÄ‚îÄ mcp_server.py               # Servidor MCP completo com Tools, Resources e Prompts
‚îú‚îÄ‚îÄ graphs/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ graph.py                # L√≥gica isolada do LangGraph (grafo, n√≥s, estados)
‚îú‚îÄ‚îÄ providers/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ llm_providers.py        # Sistema modular de provedores LLM
‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ tools.py                # Ferramentas de busca vetorial e weather
‚îú‚îÄ‚îÄ transform/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ lang_mcp_transform.py   # Bridge entre LangChain e MCP
‚îú‚îÄ‚îÄ logger/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ logger.py               # Sistema de logging colorido
‚îú‚îÄ‚îÄ .env                        # Configura√ß√µes do ambiente (n√£o commitado)
‚îú‚îÄ‚îÄ .env.example               # Exemplo de configura√ß√µes
‚îî‚îÄ‚îÄ requirements.txt           # Depend√™ncias do projeto
```

## üåê Integra√ß√£o MCP

### MCP Tools
- `search_knowledge_base`: Busca na base vetorial ChromaDB
- `get_weather`: Informa√ß√µes meteorol√≥gicas atuais
- `langgraph_orchestrator`: Agente completo com orquestra√ß√£o inteligente

### MCP Resources
- `config://agent`: Configura√ß√£o do agente e ferramentas dispon√≠veis
- `knowledge://base`: Informa√ß√µes sobre a base de conhecimento vetorial
- `status://system`: Status geral do sistema (servidor, agente, tools)

### MCP Prompts
- `agent-query`: Template flex√≠vel para consultas (estilos: conversational, technical, concise, educational)
- `knowledge-search`: Template otimizado para busca na base de conhecimento
- `weather-query`: Template para consultas meteorol√≥gicas
- `tool-orchestration`: Template para tarefas que precisam de m√∫ltiplas ferramentas

### Usando MCP Inspector
1. Execute o servidor MCP: `fastmcp run mcp_server.py --transport streamable-http --host 127.0.0.1 --port 8088`
2. Execute o MCP Inspector: `mcp-inspector`
3. Abra o navegador em `http://localhost:3000`
4. Conecte em: `http://127.0.0.1:8088/mcp`
5. Explore Tools, Resources e Prompts na interface web

## üèóÔ∏è Arquitetura

### Componentes Principais

1. **Agent (agent.py)**
   - Interface de linha de comando
   - Loop interativo para conversa√ß√£o
   - Inicializa√ß√£o e configura√ß√£o do sistema

2. **Graph (graphs/graph.py)**
   - Implementa√ß√£o do grafo LangGraph
   - N√≥s de decis√£o e execu√ß√£o 
   - Estado do agente e fluxo de mensagens
   - Orquestra√ß√£o entre ferramentas e resposta direta

3. **Providers (providers/llm_providers.py)**
   - Sistema modular de provedores LLM
   - Suporte para Ollama, OpenAI, Gemini
   - Configura√ß√£o autom√°tica via vari√°veis de ambiente

4. **Tools (tools/tools.py)**
   - Ferramentas LangChain (busca vetorial, clima)
   - Integra√ß√£o com ChromaDB e APIs externas

5. **Transform (transform/lang_mcp_transform.py)**
   - Bridge entre LangChain e MCP
   - Convers√£o autom√°tica de ferramentas
   - Wrapper ass√≠ncrono para compatibilidade

6. **MCP Server (mcp_server.py)**
   - Servidor MCP completo
   - Exposi√ß√£o de Tools, Resources e Prompts
   - Interface web via MCP Inspector

### Fluxo de Dados

```mermaid
graph TD
    A[Cliente MCP] --> B[MCP Server]
    B --> C[Transform Bridge]
    C --> D[LangGraph Agent]
    D --> E[LLM Provider]
    D --> F[Tools]
    F --> G[ChromaDB]
    F --> H[Weather API]
    
    B --> I[Resources]
    B --> J[Prompts]
    
    I --> K[Config]
    I --> L[Status] 
    I --> M[Knowledge Base Info]
```

### Decis√£o de Orquestra√ß√£o

O agente decide automaticamente entre:

1. **Resposta Direta**: Para conversas gerais
2. **Busca na Base**: Para t√≥picos t√©cnicos (python, IA, etc.)
3. **Ferramentas Externas**: Para clima, c√°lculos espec√≠ficos
4. **Combina√ß√£o**: Usar m√∫ltiplas ferramentas quando necess√°rio